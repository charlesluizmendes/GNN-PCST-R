{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "815b51db",
   "metadata": {},
   "source": [
    "## Build Snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e979943",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea8cb26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import bz2\n",
    "import csv\n",
    "import statistics\n",
    "import torch\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a906102d",
   "metadata": {},
   "source": [
    "### Open Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28cd3c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_text(path):\n",
    "    p = str(path)\n",
    "    if p.endswith(\".bz2\"):\n",
    "        return bz2.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2db83b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_records(path):\n",
    "    with open_text(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"): continue\n",
    "            parts = line.split(\"|\")\n",
    "            if len(parts) < 3: continue\n",
    "            yield int(parts[0]), int(parts[1]), int(parts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1afb5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snapshot_name(path):\n",
    "    name = Path(path).name\n",
    "    return name.split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfad4359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adjacency_list(edge_index, num_nodes):\n",
    "    adj = [set() for _ in range(num_nodes)]\n",
    "    for i in tqdm(range(edge_index.size(1)), desc=\"build_snapshots: construindo grafo\", unit=\"aresta\"):\n",
    "        u = int(edge_index[0, i].item())\n",
    "        v = int(edge_index[1, i].item())\n",
    "        adj[u].add(v); adj[v].add(u)\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e20d8dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_server(adj, num_nodes):\n",
    "    degrees = [len(adj[node]) for node in range(num_nodes)]\n",
    "    server_id = degrees.index(max(degrees))\n",
    "    print(f\"  -> Servidor: node_id={server_id}, grau={degrees[server_id]}\")\n",
    "    return server_id, degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4f58df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_peripheral_nodes_adaptive(adj, num_nodes, degrees):\n",
    "    valid_degrees = [d for d in degrees if d > 0]\n",
    "    if len(valid_degrees) >= 4:\n",
    "        q1 = statistics.quantiles(valid_degrees, n=4)[0]\n",
    "    elif len(valid_degrees) > 0:\n",
    "        q1 = float(min(valid_degrees))\n",
    "    else:\n",
    "        q1 = 0.0\n",
    "\n",
    "    neighbor_degree_by_node = []\n",
    "    for node in range(num_nodes):\n",
    "        if degrees[node] > 0:\n",
    "            neighbor_degrees = [degrees[n] for n in adj[node]]\n",
    "            if neighbor_degrees:\n",
    "                neighbor_degree_by_node.append(sum(neighbor_degrees) / len(neighbor_degrees))\n",
    "    \n",
    "    median_neighbor_degree = statistics.median(neighbor_degree_by_node) if neighbor_degree_by_node else 0\n",
    "    \n",
    "    peripheral = set()\n",
    "    for node in tqdm(range(num_nodes), desc=\"build_snapshots: classificando antenas\", unit=\"nó\"):\n",
    "        degree = degrees[node]\n",
    "        if degree == 0: continue\n",
    "        if degree == 1:\n",
    "            peripheral.add(node)\n",
    "        elif degree <= q1:\n",
    "            vals = [degrees[n] for n in adj[node]]\n",
    "            if vals and (sum(vals) / len(vals)) > median_neighbor_degree:\n",
    "                peripheral.add(node)\n",
    "    \n",
    "    print(f\"  -> Antenas identificadas: {len(peripheral)} nós\")\n",
    "    return peripheral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca5e38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_node_types(edge_index, num_nodes):\n",
    "    adj = build_adjacency_list(edge_index, num_nodes)\n",
    "    server_id, degrees = identify_server(adj, num_nodes)\n",
    "    peripheral = identify_peripheral_nodes_adaptive(adj, num_nodes, degrees)\n",
    "    \n",
    "    node_types = {}\n",
    "    for node in range(num_nodes):\n",
    "        if node == server_id: node_types[node] = \"servidor\"\n",
    "        elif node in peripheral: node_types[node] = \"antena\"\n",
    "        else: node_types[node] = \"roteador\"\n",
    "    return node_types, server_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9de8ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edge_index_from_file(path, asn2id):\n",
    "    src, dst, et = [], [], []\n",
    "    for a, b, rel in iter_records(path):\n",
    "        ia, ib = asn2id[a], asn2id[b]\n",
    "        if rel == 0:\n",
    "            src.extend([ia, ib]); dst.extend([ib, ia]); et.extend([0, 0])\n",
    "        elif rel == -1:\n",
    "            src.extend([ia, ib]); dst.extend([ib, ia]); et.extend([1, 2])\n",
    "            \n",
    "    return torch.tensor([src, dst], dtype=torch.long), torch.tensor(et, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1356bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build_snapshots: salvando snapshots.txt: 100%|██████████| 120/120 [00:00<00:00, 232264.18snapshot/s]\n",
      "build_snapshots: construindo grafo: 100%|██████████| 922674/922674 [00:03<00:00, 268267.99aresta/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Servidor: node_id=131, grau=4889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build_snapshots: classificando antenas: 100%|██████████| 101606/101606 [00:00<00:00, 4590085.11nó/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Antenas identificadas: 18178 nós\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build_snapshots: gerando grafos .pt: 100%|██████████| 120/120 [01:34<00:00,  1.28snapshot/s]\n",
      "build_snapshots: salvando nodes.csv: 100%|██████████| 101606/101606 [00:00<00:00, 1343848.00nó/s]\n"
     ]
    }
   ],
   "source": [
    "input_dir = Path(\"../../inputs\")\n",
    "output_dir = Path(\"../../outputs\")\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "files = sorted(list(input_dir.glob(\"*.txt\")) + list(input_dir.glob(\"*.txt.bz2\")))\n",
    "if not files: \n",
    "    print(\"Nenhum arquivo encontrado!\")\n",
    "else:\n",
    "    nodes = set()\n",
    "    for p in files:\n",
    "        for a, b, _ in iter_records(p): \n",
    "            nodes.add(a)\n",
    "            nodes.add(b)\n",
    "    \n",
    "    asn2id = {asn: i for i, asn in enumerate(sorted(nodes))}\n",
    "    \n",
    "    with open(output_dir / \"snapshots.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in tqdm(files, desc=\"build_snapshots: salvando snapshots.txt\", unit=\"snapshot\"):\n",
    "            f.write(snapshot_name(p) + \"\\n\")\n",
    "    \n",
    "    edge_index_first, _ = build_edge_index_from_file(files[0], asn2id)\n",
    "    node_types, server_id = identify_node_types(edge_index_first, len(asn2id))\n",
    "    \n",
    "    last_snapshot_name = None\n",
    "    for p in tqdm(files, desc=\"build_snapshots: gerando grafos .pt\", unit=\"snapshot\"):\n",
    "        edge_index, edge_type = build_edge_index_from_file(p, asn2id)\n",
    "        current_name = snapshot_name(p)\n",
    "        ts = int(re.search(r'\\d+', current_name).group()) if re.search(r'\\d+', current_name) else 0\n",
    "        obj = {\n",
    "            \"snapshot\": current_name,\n",
    "            \"timestamp\": ts,\n",
    "            \"prev_snapshot\": last_snapshot_name,\n",
    "            \"num_nodes\": len(asn2id),\n",
    "            \"edge_index\": edge_index,\n",
    "            \"edge_type\": edge_type,\n",
    "            \"server_id\": server_id,\n",
    "            \"node_types\": node_types\n",
    "        }\n",
    "        torch.save(obj, output_dir / f\"as_graph_{current_name}.pt\")\n",
    "        last_snapshot_name = current_name\n",
    "    \n",
    "    with open(output_dir / \"nodes.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"node_id\", \"asn\", \"tipo\"])\n",
    "        for asn, node_id in tqdm(asn2id.items(), desc=\"build_snapshots: salvando nodes.csv\", unit=\"nó\"):\n",
    "            w.writerow([node_id, asn, node_types.get(node_id, \"roteador\")])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
